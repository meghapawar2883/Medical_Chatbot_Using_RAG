# -*- coding: utf-8 -*-
"""Medical_Chatbot_UsingRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EuEUBiLupCB9N8hIqhXBFFoBGczCHTtF

Load the necessary
"""

!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain
#

loader = PyPDFDirectoryLoader("/content")
docs = loader.load()

len(docs) #no. of pages

docs[6]

"""Chunking

"""

Text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = Text_splitter.split_documents(docs)
#

len(chunks)

chunks[150]

chunks[151]

"""Embeddings Creation

"""

import os
os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'

embeddings = SentenceTransformerEmbeddings(model_name="NeuML/pubmedbert-base-embeddings")

"""Vector Store Creation

"""

vectorstore = Chroma.from_documents(chunks, embeddings)
#

query = "Who is at risk of heart disease?"
search_results = vectorstore.similarity_search(query)

search_results

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

retriever.get_relevant_documents(query)
#

"""LLM Model Loading

"""

llm = LlamaCpp(
    model_path="/content/drive/MyDrive/Colab Notebooks/GenAI/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.2,
    max_tokens = 2048,
    top_p = 1

)

"""Use LLM and retriver & query, to generate final response

"""

template = """
<|context|>
You are an medical Assistant that follows the instructions ans generate the accurate response based on the query and the context provided.
Please be truthful and give direct answers.
</s>
<|user|>
{query}
</s>
<|assistant|>
"""

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}  # Assuming you want to use a retriever here for context.
    | prompt
    | llm
    | StrOutputParser()
)

response = rag_chain.invoke(query)

response

import sys

while True:
  user_input = input(f"Input Query: ")
  if user_input == "exit":
    sys.exit()
  if user_input=="":
    continue
  result = rag_chain.invoke(user_input)
  print("Answer: ",result)

from google.colab import drive
drive.mount('/content/drive')